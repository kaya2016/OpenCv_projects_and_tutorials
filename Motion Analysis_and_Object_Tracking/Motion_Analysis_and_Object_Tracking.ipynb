{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Analysis and Object Tracking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Color:\n",
    "It is a very useful technique when we doing tracking because if we know the color of the object we can easily seprated from the rest of the scene.\n",
    "\n",
    "### Filter by Hue:\n",
    "The colors are stored as Hues which are rrepresented in cylindricla representation. That makes the filtiren by color easier \n",
    "* Color Range Filters:\n",
    "      * Red - 165 to 15\n",
    "      * Green - 45 to 75\n",
    "      * Blue - 90 to 120\n",
    "      \n",
    "\n",
    "### Filter by Saturation and value/Brightness:\n",
    "* Typically we set the filter range from 50 to 255 for both saturation and value as this capture the actual colors \n",
    "* 0 to 60 in saturation is close to white \n",
    "* 0 to 60 in value is close to black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background subtractors – KNN, MOG2, and GMG\n",
    "OpenCV provides a class called BackgroundSubtractor, which is a handy way to operate foreground and background segmentation.\n",
    "BackgroundSubtractor provides methodes that improve background detection in time through machine learning and lets you save the classifier to a file.\n",
    "\n",
    "BackgroundSubtractor classes are specifically built with video analysis in mind, which means that the OpenCV BackgroundSubtractor classes \"learn\" something about the environment with every frame.\n",
    "\n",
    "Another fundamental (and frankly, quite amazing) feature of the BackgroundSubtractor classes is the ability to compute shadows.\n",
    "This greatly reduces the unwanted \"merging\" of objects.\n",
    "\n",
    "Here is an example of background subtaction wihtout shadow detecetion (without shadows thresholded): The image in the right shows us an example of shadow detection.\n",
    "\n",
    "![](images/Shadow.png)\n",
    "\n",
    "Note that shadow detection isn't absolutely perfect, but it helps bring the object contours back to the object's original shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV 3.1.0\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    cv2.imshow('frame',fgmask)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meanshift – An Object Tracking Algorithm:\n",
    "Background subtraction is a really effective technique, but not the only one available to track objects in a video. Meanshift is an algorithm that tracks objects by finding the maximum density of a discrete sample of a probability function (in our case, a region of interest in an image) and recalculating it at the next frame, which gives the algorithm an indication of the direction in which the object has moved.  \n",
    "This calculation gets repeated until the centroid matches the original one, or remains unaltered even after consecutive iterations of the calculation. This final matching is called convergence.\n",
    "\n",
    "Meanshift is very useful when tracking a particular region of interest in a video, and this has a series of implications; for example, if you don't know a priori what the region you want to track is, you're going to have to manage this cleverly and develop programs that dynamically start tracking (and cease tracking) certain areas of the video, depending on arbitrary criteria. One example could be that you operate object detection with a trained SVM, and then start using Meanshift to track a detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "print (type(frame))\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([0,0,0])\n",
    "upper_purple = np.array([10,10,10])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        \n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "\n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAMShift\n",
    "Camshift is very similar to Meanshift, however you may have noted the window in Meanshift is of a fixed size. That is problematic since movement in images can be small or large. If the window is too large, you can miss the object when tracking.\n",
    "\n",
    "Camshift (Continuously Adaptive Meanshift) uses an adaptive window size that changes both size and orientation (i.e. rotates). We’ll simply it’s steps here: \n",
    "1. Applies Meanshift till it converges \n",
    "2. Calculates the size of the window \n",
    "3. Calculates the orientation by using the best fitting ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "# setup initial location of window\n",
    "r,h,c,w = 300,200,400,300  # simply hardcoded the values\n",
    "track_window = (c,r,w,h)\n",
    "\n",
    "\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "mask = cv2.inRange(hsv_roi, np.array((100., 30.,32.)), np.array((180.,120.,255.)))\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while(1):\n",
    "    ret ,frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame,[pts],True, 255,2)\n",
    "\n",
    "        cv2.imshow('img2',img2)\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
